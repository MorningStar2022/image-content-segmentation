import numpy as np
import torch
import warnings
import json
from pathlib import Path
from tqdm import tqdm
from PIL import Image
import cv2
import os
import glob
import argparse
import sys
import time
from typing import Optional

# -------------------------- å¯¼å…¥æ¨¡å‹ç›¸å…³æ¨¡å— --------------------------
from FastSAM.fastsam import FastSAM, FastSAMPrompt
from Hi_SAM.hi_sam.modeling.build import model_registry
from Hi_SAM.hi_sam.modeling.predictor import SamPredictor

warnings.filterwarnings("ignore")


# -------------------------- COCOæ ¼å¼å·¥å…·å‡½æ•° --------------------------
# -------------------------- COCOæ ¼å¼å·¥å…·å‡½æ•°ï¼ˆä¿®å¤ç‰ˆï¼‰ --------------------------
def init_coco_format():
    """åˆå§‹åŒ–COCOæ ¼å¼æ•°æ®ç»“æ„"""
    return {
        "info": {},
        "licenses": [],
        "categories": [
            {"id": 1, "name": "text", "supercategory": "object"},
            {"id": 2, "name": "edge", "supercategory": "object"},
            {"id": 3, "name": "object", "supercategory": "object"}
        ],
        "images": [],
        "annotations": []
    }


def mask_to_coco_rle(mask):
    """å°†äºŒå€¼æ©ç è½¬æ¢ä¸ºCOCO RLEæ ¼å¼ï¼ˆä¿®å¤å…¨0æ©ç é—®é¢˜ï¼‰"""
    mask = mask.astype(np.uint8)
    rle = {"counts": [], "size": list(mask.shape)}
    counts = []
    prev = 0

    # éå†æ‰å¹³åŒ–åçš„æ©ç ï¼ˆFortran orderï¼Œåˆ—ä¼˜å…ˆï¼‰
    for pixel in mask.flatten(order='F'):
        if pixel != prev:
            # åƒç´ å€¼å˜åŒ–ï¼Œæ–°å¢è®¡æ•°é¡¹
            counts.append(1)
            prev = pixel
        else:
            # åƒç´ å€¼ä¸å˜ï¼Œç´¯åŠ è®¡æ•°ï¼ˆå¤„ç†ç©ºåˆ—è¡¨æƒ…å†µï¼‰
            if counts:  # ä»…å½“countséç©ºæ—¶ç´¯åŠ 
                counts[-1] += 1
            else:  # ç©ºåˆ—è¡¨è¯´æ˜æ˜¯ç¬¬ä¸€ä¸ªåƒç´ ä¸”ä¸º0ï¼Œåˆå§‹åŒ–è®¡æ•°
                counts.append(1)

    # å¤„ç†å…¨0æ©ç çš„ç‰¹æ®Šæƒ…å†µï¼ˆcountsä¸ºç©ºæˆ–ä»…ä¸€ä¸ªå…ƒç´ ï¼‰
    if not counts:
        counts = [mask.size]  # å…¨0æ—¶ï¼Œè®¡æ•°ä¸ºæ©ç æ€»åƒç´ æ•°

    rle["counts"] = counts
    return rle


def add_coco_annotation(coco_data, img_id, mask, category_id):
    """å‘COCOæ•°æ®ä¸­æ·»åŠ æ ‡æ³¨ï¼ˆå¢å¼ºè¾¹ç•Œæ£€æŸ¥ï¼‰"""
    # 1. æ£€æŸ¥æ©ç æ˜¯å¦å…¨0ï¼Œç›´æ¥è·³è¿‡
    if np.sum(mask) == 0:
        print(f"âš ï¸ è·³è¿‡ç©ºæ©ç æ ‡æ³¨ï¼ˆç±»åˆ«ID: {category_id}ï¼Œå›¾åƒID: {img_id}ï¼‰")
        return

    # 2. è®¡ç®—æ©ç é¢ç§¯ï¼ˆ0/1æ©ç çš„å’Œå³ä¸ºé¢ç§¯ï¼‰
    area = int(np.sum(mask))

    # 3. è®¡ç®—è¾¹ç•Œæ¡† (x, y, width, height)
    where = np.argwhere(mask)
    if len(where) == 0:
        print(f"âš ï¸ æ©ç æ— æœ‰æ•ˆåƒç´ ï¼ˆç±»åˆ«ID: {category_id}ï¼Œå›¾åƒID: {img_id}ï¼‰")
        return

    y1, x1 = where.min(axis=0)
    y2, x2 = where.max(axis=0)
    bbox = [int(x1), int(y1), int(x2 - x1 + 1), int(y2 - y1 + 1)]

    # 4. ç”ŸæˆRLEç¼–ç 
    rle = mask_to_coco_rle(mask)

    # 5. åˆ›å»ºæ ‡æ³¨ï¼ˆä¿è¯annotation IDå”¯ä¸€ï¼‰
    annotation = {
        "id": len(coco_data["annotations"]) + 1,
        "image_id": img_id,
        "category_id": category_id,
        "segmentation": rle,
        "area": area,
        "bbox": bbox,
        "iscrowd": 0,
        "attributes": {}
    }

    coco_data["annotations"].append(annotation)


# -------------------------- å…¨å±€é…ç½®ä¸å·¥å…·å‡½æ•° --------------------------
def get_args_parser():
    parser = argparse.ArgumentParser('Fast-SAM + Hi-SAM + æ©ç ä¼˜åŒ– é«˜æ•ˆæµç¨‹', add_help=False)
    # é€šç”¨é…ç½®
    parser.add_argument("--input", type=str, required=True, help="è¾“å…¥å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„")
    parser.add_argument("--output", type=str, default='./final_results', help="ç»“æœä¿å­˜æ ¹ç›®å½•")
    parser.add_argument("--device", type=str, default="cuda:0", help="è¿è¡Œè®¾å¤‡")

    # Fast-SAMé…ç½®
    parser.add_argument("--fastsam_checkpoint", type=str, required=True, help="Fast-SAMæƒé‡è·¯å¾„")
    parser.add_argument("--fastsam_conf", type=float, default=0.4, help="Fast-SAMç½®ä¿¡åº¦é˜ˆå€¼")
    parser.add_argument("--fastsam_iou", type=float, default=0.9, help="Fast-SAM IoUé˜ˆå€¼")
    parser.add_argument("--fastsam_imgsz", type=int, default=640, help="Fast-SAMè¾“å…¥å›¾åƒå°ºå¯¸")

    # Hi-SAMé…ç½®
    parser.add_argument("--hisam_model_type", type=str, default="vit_l",
                        help="Hi-SAMæ¨¡å‹ç±»å‹ ['vit_h', 'vit_l', 'vit_b']")
    parser.add_argument("--hisam_checkpoint", type=str, required=True, help="Hi-SAMæƒé‡è·¯å¾„")
    parser.add_argument("--hisam_hier_det", action='store_true', help="Hi-SAMæ˜¯å¦å¯ç”¨å±‚çº§æ£€æµ‹")
    parser.add_argument("--hisam_patch_mode", action='store_true', help="Hi-SAMæ˜¯å¦å¯ç”¨patchæ¨¡å¼")
    parser.add_argument('--input_size', default=[1024, 1024], type=list)
    parser.add_argument('--attn_layers', default=1, type=int, help='cross attention layersæ•°')
    parser.add_argument('--prompt_len', default=12, type=int, help='prompt tokenæ•°')

    # åå¤„ç†é…ç½®
    parser.add_argument("--text_dilate_pixel", type=int, default=20, help="æ–‡æœ¬æ©ç è†¨èƒ€åƒç´ æ•°")
    parser.add_argument("--edge_white_value", type=int, default=255, help="è¾¹ç¼˜æ©ç ç™½è‰²å€¼")
    parser.add_argument("--fill_black_value", type=int, default=0, help="é‡å åŒºåŸŸå¡«å……é»‘è‰²å€¼")

    return parser.parse_args()


# -------------------------- Hi-SAMå·¥å…·å‡½æ•° --------------------------
def patchify_sliding(image: np.array, patch_size: int = 512, stride: int = 256):
    h, w = image.shape[:2]
    patch_list = []
    h_slice_list = []
    w_slice_list = []
    for j in range(0, h, stride):
        start_h, end_h = j, j + patch_size
        if end_h > h:
            start_h = max(h - patch_size, 0)
            end_h = h
        for i in range(0, w, stride):
            start_w, end_w = i, i + patch_size
            if end_w > w:
                start_w = max(w - patch_size, 0)
                end_w = w
            h_slice = slice(start_h, end_h)
            h_slice_list.append(h_slice)
            w_slice = slice(start_w, end_w)
            w_slice_list.append(w_slice)
            patch_list.append(image[h_slice, w_slice])
    return patch_list, h_slice_list, w_slice_list


def unpatchify_sliding(patch_list, h_slice_list, w_slice_list, ori_size):
    assert len(ori_size) == 2
    whole_logits = np.zeros(ori_size)
    assert len(patch_list) == len(h_slice_list) == len(w_slice_list)
    for idx in range(len(patch_list)):
        h_slice = h_slice_list[idx]
        w_slice = w_slice_list[idx]
        whole_logits[h_slice, w_slice] += patch_list[idx]
    return whole_logits


# -------------------------- æ©ç ä¼˜åŒ–å‡½æ•°ï¼ˆæ— æ–‡ä»¶IOï¼‰ --------------------------
def refine_edge_mask(
        edge_mask: np.ndarray,
        text_mask: Optional[np.ndarray] = None,
        edge_white_value: int = 255,
        fill_black_value: int = 0,
        text_dilate_pixel: int = 20
) -> np.ndarray:
    """ä¼˜åŒ–SAMè¾¹ç¼˜æ©ç ï¼šçº¯å†…å­˜æ“ä½œï¼Œä¸æ¶‰åŠæ–‡ä»¶è¯»å†™"""
    # æ­¥éª¤1ï¼šç»Ÿä¸€è¾¹ç¼˜æ©ç ä¸ºå•é€šé“äºŒå€¼æ ¼å¼
    if len(edge_mask.shape) == 3:
        edge_mask_gray = cv2.cvtColor(edge_mask, cv2.COLOR_BGR2GRAY)
    else:
        edge_mask_gray = edge_mask.copy()
    _, edge_mask_bin = cv2.threshold(
        edge_mask_gray,
        edge_white_value - 1,
        edge_white_value,
        cv2.THRESH_BINARY
    )

    # æ­¥éª¤2ï¼šåˆå§‹åŒ–ä¼˜åŒ–åçš„è¾¹ç¼˜æ©ç 
    refined_edge_mask = edge_mask_bin.copy()

    # æ­¥éª¤3ï¼šå¤„ç†æ–‡æœ¬æ©ç ï¼ˆæ ¸å¿ƒï¼‰
    if text_mask is not None:
        # æ–‡æœ¬æ©ç è½¬å•é€šé“äºŒå€¼
        if len(text_mask.shape) == 3:
            text_mask_gray = cv2.cvtColor(text_mask, cv2.COLOR_BGR2GRAY)
        else:
            text_mask_gray = text_mask.copy()
        _, text_mask_bin = cv2.threshold(text_mask_gray, 1, 255, cv2.THRESH_BINARY)

        # æ–‡æœ¬æ©ç è†¨èƒ€
        dilate_kernel = np.ones((text_dilate_pixel * 2 + 1, text_dilate_pixel * 2 + 1), np.uint8)
        text_mask_dilated = cv2.dilate(text_mask_bin, dilate_kernel, iterations=1)

        # é‡å åŒºåŸŸæ¶‚é»‘
        text_edge_overlap = np.logical_and(edge_mask_bin == edge_white_value, text_mask_dilated == 255)
        refined_edge_mask[text_edge_overlap] = fill_black_value

    return refined_edge_mask


# -------------------------- æ¨¡å‹æ¨ç†å‡½æ•°ï¼ˆæ¯«ç§’å•ä½æ—¶é—´ç»Ÿè®¡ï¼‰ --------------------------
def run_fastsam_inference(img_path, fastsam_model, device, imgsz=1024, conf=0.4, iou=0.9):
    """Fast-SAMæ¨ç†ï¼šè¿”å›è¾¹ç¼˜æ©ç æ•°ç»„ã€åŸå§‹ç‰©ä½“æ©ç åˆ—è¡¨ + æ¨ç†è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰"""
    try:
        # è®°å½•Fast-SAMæ¨ç†å¼€å§‹æ—¶é—´
        start_time = time.time()

        # è¯»å–å›¾åƒ
        image = cv2.imread(img_path)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        img_h, img_w = image.shape[:2]

        # ç”Ÿæˆæ‰€æœ‰æ©ç ï¼ˆä½¿ç”¨everything promptï¼‰
        everything_results = fastsam_model(
            img_path,
            device=device,
            retina_masks=True,
            imgsz=imgsz,
            conf=conf,
            iou=iou
        )

        # å¤„ç†ç»“æœè·å–æ‰€æœ‰æ©ç 
        prompt_process = FastSAMPrompt(img_path, everything_results, device=device)
        ann = prompt_process.everything_prompt()  # è·å–æ‰€æœ‰æ©ç 

        # ç”Ÿæˆè¾¹ç¼˜æ©ç ï¼ˆä»…å†…å­˜æ“ä½œï¼‰
        edge_mask = np.zeros(image.shape[:2], dtype=np.uint8)
        object_masks = []  # å­˜å‚¨æ‰€æœ‰åŸå§‹ç‰©ä½“æ©ç 

        for mask in ann:
            # å°†æ©ç è½¬æ¢ä¸ºäºŒå€¼å›¾åƒ
            mask_np = mask.cpu().numpy().astype(np.uint8) * 255
            object_masks.append(mask_np)  # ä¿å­˜åŸå§‹ç‰©ä½“æ©ç 

            # æå–è¾¹ç¼˜
            edges = cv2.Canny(mask_np, threshold1=50, threshold2=150)
            # åˆå¹¶åˆ°è¾¹ç¼˜æ©ç 
            edge_mask = cv2.bitwise_or(edge_mask, edges)

        # è®¡ç®—Fast-SAMæ¨ç†è€—æ—¶ï¼ˆè½¬æ¢ä¸ºæ¯«ç§’ï¼Œä¿ç•™1ä½å°æ•°ï¼‰
        fastsam_infer_time = round((time.time() - start_time) * 1000, 1)

        # æ¸…ç†æ˜¾å­˜ç¼“å­˜ï¼ˆç¼“è§£OOMï¼‰
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return {
            "status": "success",
            "img_name": Path(img_path).stem,
            "img_size": (img_h, img_w),
            "sam_edge_mask": edge_mask,
            "object_masks": object_masks,  # åŸå§‹ç‰©ä½“æ©ç åˆ—è¡¨
            "sam_infer_time": fastsam_infer_time  # æ¯«ç§’å•ä½
        }
    except Exception as e:
        return {
            "status": "failed",
            "img_path": img_path,
            "error": str(e),
            "sam_infer_time": 0.0  # å¤±è´¥æ—¶è€—æ—¶è®°ä¸º0
        }


def run_hisam_inference(img_path, hisam_model, hier_det=False, patch_mode=False):
    """Hi-SAMæ¨ç†ï¼šè¿”å›æ–‡æœ¬æ©ç æ•°ç»„ + æ¨ç†è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰"""
    try:
        # è®°å½•Hi-SAMæ¨ç†å¼€å§‹æ—¶é—´
        start_time = time.time()

        predictor = SamPredictor(hisam_model)
        image = cv2.imread(img_path)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        img_name = Path(img_path).stem

        if patch_mode:
            # Patchæ¨¡å¼æ¨ç†
            ori_size = image.shape[:2]
            patch_list, h_slice_list, w_slice_list = patchify_sliding(image_rgb, 512, 384)
            mask_512 = []
            for patch in patch_list:
                predictor.set_image(patch)
                m, hr_m, score, hr_score = predictor.predict(multimask_output=False, return_logits=True)
                mask_512.append(hr_m[0])
            mask_512 = unpatchify_sliding(mask_512, h_slice_list, w_slice_list, ori_size)
            text_mask = (mask_512 > predictor.model.mask_threshold).astype(np.uint8) * 255
        else:
            predictor.set_image(image_rgb)
            if hier_det:
                # å±‚çº§æ£€æµ‹æ¨¡å¼
                input_point = np.array([[125, 275]])
                input_label = np.ones(input_point.shape[0])
                mask, hr_mask, score, hr_score, hi_mask, hi_iou, word_mask = predictor.predict(
                    multimask_output=False,
                    hier_det=True,
                    point_coords=input_point,
                    point_labels=input_label,
                )
                text_mask = hr_mask[0].astype(np.uint8) * 255  # è½¬ä¸º0-255çš„å•é€šé“æ©ç 
            else:
                # æ™®é€šæ–‡æœ¬åˆ†å‰²æ¨¡å¼
                mask, hr_mask, score, hr_score = predictor.predict(multimask_output=False)
                text_mask = hr_mask[0].astype(np.uint8) * 255  # è½¬ä¸º0-255çš„å•é€šé“æ©ç 

        # è®¡ç®—Hi-SAMæ¨ç†è€—æ—¶ï¼ˆè½¬æ¢ä¸ºæ¯«ç§’ï¼Œä¿ç•™1ä½å°æ•°ï¼‰
        hisam_infer_time = round((time.time() - start_time) * 1000, 1)

        # æ¸…ç†æ˜¾å­˜ç¼“å­˜ï¼ˆç¼“è§£OOMï¼‰
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return {
            "status": "success",
            "img_name": img_name,
            "hisam_text_mask": text_mask,
            "hisam_infer_time": hisam_infer_time  # æ¯«ç§’å•ä½
        }
    except Exception as e:
        return {
            "status": "failed",
            "img_path": img_path,
            "error": str(e),
            "hisam_infer_time": 0.0  # å¤±è´¥æ—¶è€—æ—¶è®°ä¸º0
        }


# -------------------------- ä¸»å‡½æ•°ï¼ˆæ¯«ç§’å•ä½æ—¶é—´ç»Ÿè®¡æ±‡æ€»ï¼‰ --------------------------
def main():
    args = get_args_parser()

    # 1. åˆ›å»ºç»“æœç›®å½•
    os.makedirs(args.output, exist_ok=True)
    print(f"ğŸ“ ç»“æœä¿å­˜ç›®å½•ï¼š{args.output}")

    # 2. åŠ è½½æ¨¡å‹
    print("\nğŸš€ åŠ è½½æ¨¡å‹...")
    # åŠ è½½Fast-SAMæ¨¡å‹
    fastsam = FastSAM(args.fastsam_checkpoint)
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")

    # åŠ è½½Hi-SAMæ¨¡å‹
    hisam = model_registry[args.hisam_model_type](args)
    hisam.eval()
    hisam.to(device)
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡ï¼š{device}")

    # 3. è·å–è¾“å…¥å›¾åƒåˆ—è¡¨
    input_images = []
    if os.path.isdir(args.input):
        for fname in os.listdir(args.input):
            img_path = os.path.join(args.input, fname)
            if cv2.haveImageReader(img_path):
                input_images.append(img_path)
    else:
        input_images = glob.glob(os.path.expanduser(args.input))

    assert len(input_images) > 0, "âŒ æœªæ‰¾åˆ°æœ‰æ•ˆè¾“å…¥å›¾åƒ"
    print(f"\nğŸ“¸ å¾…å¤„ç†å›¾åƒæ•°é‡ï¼š{len(input_images)}")

    # åˆå§‹åŒ–æ—¶é—´ç»Ÿè®¡å˜é‡ï¼ˆæ¯«ç§’ï¼‰
    total_sam_time = 0.0  # Fast-SAMæ€»è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
    total_hisam_time = 0.0  # Hi-SAMæ€»è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
    success_sam_count = 0  # Fast-SAMæˆåŠŸæ•°
    success_hisam_count = 0  # Hi-SAMæˆåŠŸæ•°
    time_stats = []  # å•å¼ å›¾ç‰‡è€—æ—¶æ˜ç»†

    # 4. ä¸²è¡Œè¿è¡ŒFast-SAM + Hi-SAMæ¨ç†
    print("\nâš¡ å¼€å§‹ä¸²è¡Œæ¨ç†ï¼ˆFast-SAM + Hi-SAMï¼‰...")
    inference_results = {}
    success_count = 0

    # é€ä¸ªå¤„ç†æ¯å¼ å›¾ç‰‡
    for img_idx, img_path in enumerate(tqdm(input_images, desc="æ¨ç†+ä¼˜åŒ–è¿›åº¦")):
        img_name = Path(img_path).stem
        inference_results[img_name] = {}

        # åˆå§‹åŒ–COCOæ ¼å¼æ•°æ®
        coco_data = init_coco_format()

        # æ·»åŠ å›¾åƒä¿¡æ¯åˆ°COCOæ•°æ®
        img = cv2.imread(img_path)
        img_h, img_w = img.shape[:2]
        coco_data["images"].append({
            "id": img_idx + 1,
            "width": img_w,
            "height": img_h,
            "file_name": os.path.basename(img_path)
        })

        # 4.1 ä¸²è¡Œæ‰§è¡ŒFast-SAMæ¨ç†
        sam_result = run_fastsam_inference(
            img_path=img_path,
            fastsam_model=fastsam,
            device=device,
            imgsz=args.fastsam_imgsz,
            conf=args.fastsam_conf,
            iou=args.fastsam_iou
        )
        inference_results[img_name]["sam"] = sam_result

        # ç´¯åŠ Fast-SAMè€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
        if sam_result["status"] == "success":
            total_sam_time += sam_result["sam_infer_time"]
            success_sam_count += 1

        # 4.2 ä¸²è¡Œæ‰§è¡ŒHi-SAMæ¨ç†
        hisam_result = run_hisam_inference(
            img_path=img_path,
            hisam_model=hisam,
            hier_det=args.hisam_hier_det,
            patch_mode=args.hisam_patch_mode
        )
        inference_results[img_name]["hisam"] = hisam_result

        # ç´¯åŠ Hi-SAMè€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
        if hisam_result["status"] == "success":
            total_hisam_time += hisam_result["hisam_infer_time"]
            success_hisam_count += 1

        # è®°å½•å•å¼ å›¾ç‰‡è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
        time_stats.append({
            "img_name": img_name,
            "sam_time": sam_result["sam_infer_time"],
            "hisam_time": hisam_result["hisam_infer_time"],
            "sam_status": sam_result["status"],
            "hisam_status": hisam_result["status"]
        })

        # 4.3 æ©ç ä¼˜åŒ– + ä¿å­˜ + ç”ŸæˆCOCOæ ‡æ³¨
        if sam_result["status"] == "success" and hisam_result["status"] == "success":
            # ç›´æ¥ä»å†…å­˜è·å–æ©ç æ•°ç»„
            sam_edge_mask = sam_result["sam_edge_mask"]
            hisam_text_mask = hisam_result["hisam_text_mask"]
            object_masks = sam_result["object_masks"]  # è·å–åŸå§‹ç‰©ä½“æ©ç åˆ—è¡¨
            img_h, img_w = sam_result["img_size"]

            # 1. ä¿å­˜æ–‡æœ¬æ©ç å¹¶æ·»åŠ åˆ°COCO
            text_mask_path = os.path.join(args.output, f"{img_name}_hisam_text_mask.png")
            cv2.imwrite(text_mask_path, hisam_text_mask)
            # æ–‡æœ¬æ©ç äºŒå€¼åŒ–ï¼ˆ0å’Œ1ï¼‰
            text_mask_bin = (hisam_text_mask > 127).astype(np.uint8)
            add_coco_annotation(coco_data, img_idx + 1, text_mask_bin, 1)  # ç±»åˆ«1: text

            # 2. å†…å­˜ä¸­ä¼˜åŒ–è¾¹ç¼˜æ©ç å¹¶æ·»åŠ åˆ°COCO
            refined_edge_mask = refine_edge_mask(
                edge_mask=sam_edge_mask,
                text_mask=hisam_text_mask,
                edge_white_value=args.edge_white_value,
                fill_black_value=args.fill_black_value,
                text_dilate_pixel=args.text_dilate_pixel
            )
            refined_mask_path = os.path.join(args.output, f"{img_name}_refined_edge_mask.png")
            cv2.imwrite(refined_mask_path, refined_edge_mask)
            # è¾¹ç¼˜æ©ç äºŒå€¼åŒ–ï¼ˆ0å’Œ1ï¼‰
            edge_mask_bin = (refined_edge_mask > 127).astype(np.uint8)
            add_coco_annotation(coco_data, img_idx + 1, edge_mask_bin, 2)  # ç±»åˆ«2: edge

            # 3. å¤„ç†ç‰©ä½“æ©ç ï¼ˆæ’é™¤æ–‡æœ¬å’Œè¾¹ç¼˜åŒºåŸŸï¼‰å¹¶æ·»åŠ åˆ°COCO
            # åˆ›å»ºæ–‡æœ¬å’Œè¾¹ç¼˜çš„è”åˆæ©ç ï¼ˆéœ€è¦æ’é™¤çš„åŒºåŸŸï¼‰
            text_mask_dilated = cv2.dilate(
                text_mask_bin,
                np.ones((5, 5), np.uint8),
                iterations=1
            )
            exclude_mask = np.logical_or(text_mask_dilated, edge_mask_bin).astype(np.uint8)

            # åˆå¹¶æ‰€æœ‰ç‰©ä½“æ©ç å¹¶æ’é™¤æ–‡æœ¬å’Œè¾¹ç¼˜åŒºåŸŸ
            combined_object_mask = np.zeros((img_h, img_w), dtype=np.uint8)
            for mask in object_masks:
                mask_bin = (mask > 127).astype(np.uint8)
                combined_object_mask = np.logical_or(combined_object_mask, mask_bin).astype(np.uint8)

                # æ­¥éª¤2ï¼šåˆ›å»ºæ’é™¤æ©ç ï¼ˆæ–‡æœ¬+è¾¹ç¼˜ï¼Œæ–‡æœ¬åšè½»å¾®è†¨èƒ€ï¼‰
            text_mask_dilated = cv2.dilate(text_mask_bin, np.ones((5, 5), np.uint8), iterations=1)
            exclude_mask = np.logical_or(text_mask_dilated, edge_mask_bin).astype(np.uint8)

            # æ­¥éª¤3ï¼šä¸€æ¬¡æ€§æ’é™¤æ–‡æœ¬å’Œè¾¹ç¼˜åŒºåŸŸ
            combined_object_mask = np.logical_and(combined_object_mask, 1 - exclude_mask).astype(np.uint8)

            # ä¿å­˜ç‰©ä½“æ©ç 
            object_mask_path = os.path.join(args.output, f"{img_name}_object_mask.png")
            cv2.imwrite(object_mask_path, combined_object_mask * 255)
            # æ·»åŠ åˆ°COCOæ ‡æ³¨
            add_coco_annotation(coco_data, img_idx + 1, combined_object_mask, 3)  # ç±»åˆ«3: object

            # 4. ä¿å­˜COCOæ ¼å¼JSONæ–‡ä»¶
            coco_json_path = os.path.join(args.output, f"{img_name}_coco_annotations.json")
            with open(coco_json_path, 'w', encoding='utf-8') as f:
                json.dump(coco_data, f, ensure_ascii=False, indent=2)

            inference_results[img_name]["refined_edge_mask_path"] = refined_mask_path
            inference_results[img_name]["hisam_text_mask_path"] = text_mask_path
            inference_results[img_name]["object_mask_path"] = object_mask_path
            inference_results[img_name]["coco_json_path"] = coco_json_path
            success_count += 1
        else:
            # æ‰“å°å¤±è´¥ä¿¡æ¯
            print(f"\nâš ï¸ è·³è¿‡{img_name}ï¼šFast-SAM/Hi-SAMæ¨ç†å¤±è´¥")
            if sam_result["status"] == "failed":
                print(f"   - Fast-SAMå¤±è´¥åŸå› ï¼š{sam_result['error']}")
            if hisam_result["status"] == "failed":
                print(f"   - Hi-SAMå¤±è´¥åŸå› ï¼š{hisam_result['error']}")

        # å¼ºåˆ¶æ¸…ç†æ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()

    # 5. æ—¶é—´ç»Ÿè®¡ç»“æœè¾“å‡ºï¼ˆæ¯«ç§’å•ä½ï¼‰
    print("\n" + "-" * 60)
    print("ğŸ“Š æ¨ç†æ—¶é—´ç»Ÿè®¡ï¼ˆå•ä½ï¼šæ¯«ç§’ msï¼‰")
    print("-" * 60)
    # æ•´ä½“ç»Ÿè®¡
    print(f"æ€»å¤„ç†å›¾ç‰‡æ•°ï¼š{len(input_images)}")
    print(f"Fast-SAMæˆåŠŸæ¨ç†æ•°ï¼š{success_sam_count} | Hi-SAMæˆåŠŸæ¨ç†æ•°ï¼š{success_hisam_count}")
    print(f"Fast-SAMæ€»è€—æ—¶ï¼š{total_sam_time:.1f} ms | å¹³å‡æ¯å¼ ï¼š{total_sam_time / max(success_sam_count, 1):.1f} ms")
    print(f"Hi-SAMæ€»è€—æ—¶ï¼š{total_hisam_time:.1f} ms | å¹³å‡æ¯å¼ ï¼š{total_hisam_time / max(success_hisam_count, 1):.1f} ms")

    # å•å¼ å›¾ç‰‡æ˜ç»†ï¼ˆæ¯«ç§’å•ä½ï¼‰
    print("\nğŸ“‹ å•å¼ å›¾ç‰‡è€—æ—¶æ˜ç»†ï¼š")
    for stat in time_stats:
        status = f"Fast-SAM: {stat['sam_status']} | Hi-SAM: {stat['hisam_status']}"
        print(
            f"  {stat['img_name']} | Fast-SAM: {stat['sam_time']:.1f}ms | Hi-SAM: {stat['hisam_time']:.1f}ms | {status}")

    # 6. æœ€ç»ˆç»“æœè¾“å‡º
    print("\nğŸ‰ ä»»åŠ¡å®Œæˆï¼æˆåŠŸå¤„ç† "+f"{success_count}/{len(input_images)} å¼ å›¾åƒ")
    print(f"ğŸ“ ç»“æœä¿å­˜ç›®å½•ï¼š{args.output}")
    print("ğŸ“„ ä¿å­˜æ–‡ä»¶åŒ…æ‹¬ï¼š")
    print("   - {img_name}_hisam_text_mask.png: Hi-SAMæ–‡æœ¬æ©ç ")
    print("   - {img_name}_refined_edge_mask.png: ä¼˜åŒ–åçš„Fast-SAMè¾¹ç¼˜æ©ç ")
    print("   - {img_name}_object_mask.png: ç‰©ä½“æ©ç ï¼ˆæ’é™¤æ–‡æœ¬å’Œè¾¹ç¼˜ï¼‰")
    print("   - {img_name}_coco_annotations.json: COCOæ ¼å¼æ ‡æ³¨æ–‡ä»¶ï¼ˆåŒ…å«text/edge/objectä¸‰ç±»åˆ«ï¼‰")


if __name__ == '__main__':
    main()