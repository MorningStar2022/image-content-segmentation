import numpy as np
import torch
import warnings
from pathlib import Path
from tqdm import tqdm
from PIL import Image
import cv2
import os
import glob
import argparse
import sys
import time  # å¯¼å…¥æ—¶é—´æ¨¡å—
from typing import Optional

# -------------------------- å¯¼åŒ…è·¯å¾„é…ç½® --------------------------
current_script_path = os.path.abspath(__file__)
project_root = os.path.dirname(current_script_path)

hisam_source_dir = os.path.join(project_root, "Hi-SAM-main")
if hisam_source_dir not in sys.path:
    sys.path.insert(0, hisam_source_dir)

sam_source_dir = os.path.join(project_root, "segment-anything-main")
if sam_source_dir not in sys.path:
    sys.path.insert(0, sam_source_dir)


# -------------------------- å¯¼å…¥æ¨¡å‹ç›¸å…³æ¨¡å— --------------------------
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator

from hi_sam.modeling.build import model_registry
from hi_sam.modeling.predictor import SamPredictor

warnings.filterwarnings("ignore")


# -------------------------- å…¨å±€é…ç½®ä¸å·¥å…·å‡½æ•° --------------------------
def get_args_parser():
    parser = argparse.ArgumentParser('SAM + Hi-SAM + æ©ç ä¼˜åŒ– é«˜æ•ˆæµç¨‹', add_help=False)
    # é€šç”¨é…ç½®
    parser.add_argument("--input", type=str, default="./input", help="è¾“å…¥å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„")
    parser.add_argument("--output", type=str, default='./final_results', help="ç»“æœä¿å­˜æ ¹ç›®å½•")
    parser.add_argument("--device", type=str, default="cuda", help="è¿è¡Œè®¾å¤‡")

    # SAMé…ç½®
    parser.add_argument("--sam_model_type", type=str, default="vit_b", help="SAMæ¨¡å‹ç±»å‹ ['vit_t']")
    parser.add_argument("--sam_checkpoint", type=str, default="segment-anything-main/pretrain_model/sam_vit_b_01ec64.pth", help="SAMæƒé‡è·¯å¾„")
    parser.add_argument("--sam_max_masks", type=int, default=300, help="SAMæœ€å¤§æ©ç æ•°")

    # Hi-SAMé…ç½®
    parser.add_argument("--hisam_model_type", type=str, default="vit_s",
                        help="Hi-SAMæ¨¡å‹ç±»å‹ ['vit_h', 'vit_l', 'vit_b','vit_s']")
    parser.add_argument("--hisam_checkpoint", default="Hi-SAM-main/pretrained_checkpoint/efficient_hi_sam_s.pth",type=str,  help="Hi-SAMæƒé‡è·¯å¾„")
    parser.add_argument("--hisam_hier_det", default=False,action='store_true', help="Hi-SAMæ˜¯å¦å¯ç”¨å±‚çº§æ£€æµ‹")
    parser.add_argument("--hisam_patch_mode", default=False,action='store_true', help="Hi-SAMæ˜¯å¦å¯ç”¨patchæ¨¡å¼")
    parser.add_argument('--input_size', default=[1024, 1024], type=list)
    parser.add_argument('--attn_layers', default=1, type=int, help='cross attention layersæ•°')
    parser.add_argument('--prompt_len', default=12, type=int, help='prompt tokenæ•°')

    # åå¤„ç†é…ç½®
    parser.add_argument("--text_dilate_pixel", type=int, default=20, help="æ–‡æœ¬æ©ç è†¨èƒ€åƒç´ æ•°")
    parser.add_argument("--edge_white_value", type=int, default=255, help="è¾¹ç¼˜æ©ç ç™½è‰²å€¼")
    parser.add_argument("--fill_black_value", type=int, default=0, help="é‡å åŒºåŸŸå¡«å……é»‘è‰²å€¼")

    return parser.parse_args()


# -------------------------- Hi-SAMå·¥å…·å‡½æ•° --------------------------
def patchify_sliding(image: np.array, patch_size: int = 512, stride: int = 256):
    h, w = image.shape[:2]
    patch_list = []
    h_slice_list = []
    w_slice_list = []
    for j in range(0, h, stride):
        start_h, end_h = j, j + patch_size
        if end_h > h:
            start_h = max(h - patch_size, 0)
            end_h = h
        for i in range(0, w, stride):
            start_w, end_w = i, i + patch_size
            if end_w > w:
                start_w = max(w - patch_size, 0)
                end_w = w
            h_slice = slice(start_h, end_h)
            h_slice_list.append(h_slice)
            w_slice = slice(start_w, end_w)
            w_slice_list.append(w_slice)
            patch_list.append(image[h_slice, w_slice])
    return patch_list, h_slice_list, w_slice_list


def unpatchify_sliding(patch_list, h_slice_list, w_slice_list, ori_size):
    assert len(ori_size) == 2
    whole_logits = np.zeros(ori_size)
    assert len(patch_list) == len(h_slice_list) == len(w_slice_list)
    for idx in range(len(patch_list)):
        h_slice = h_slice_list[idx]
        w_slice = w_slice_list[idx]
        whole_logits[h_slice, w_slice] += patch_list[idx]
    return whole_logits


# -------------------------- æ©ç ä¼˜åŒ–å‡½æ•°ï¼ˆæ— æ–‡ä»¶IOï¼‰ --------------------------
def refine_edge_mask(
        edge_mask: np.ndarray,
        text_mask: Optional[np.ndarray] = None,
        edge_white_value: int = 255,
        fill_black_value: int = 0,
        text_dilate_pixel: int = 20
) -> np.ndarray:
    """ä¼˜åŒ–SAMè¾¹ç¼˜æ©ç ï¼šçº¯å†…å­˜æ“ä½œï¼Œä¸æ¶‰åŠæ–‡ä»¶è¯»å†™"""
    # æ­¥éª¤1ï¼šç»Ÿä¸€è¾¹ç¼˜æ©ç ä¸ºå•é€šé“äºŒå€¼æ ¼å¼
    if len(edge_mask.shape) == 3:
        edge_mask_gray = cv2.cvtColor(edge_mask, cv2.COLOR_BGR2GRAY)
    else:
        edge_mask_gray = edge_mask.copy()
    _, edge_mask_bin = cv2.threshold(
        edge_mask_gray,
        edge_white_value - 1,
        edge_white_value,
        cv2.THRESH_BINARY
    )

    # æ­¥éª¤2ï¼šåˆå§‹åŒ–ä¼˜åŒ–åçš„è¾¹ç¼˜æ©ç 
    refined_edge_mask = edge_mask_bin.copy()

    # æ­¥éª¤3ï¼šå¤„ç†æ–‡æœ¬æ©ç ï¼ˆæ ¸å¿ƒï¼‰
    if text_mask is not None:
        # æ–‡æœ¬æ©ç è½¬å•é€šé“äºŒå€¼
        if len(text_mask.shape) == 3:
            text_mask_gray = cv2.cvtColor(text_mask, cv2.COLOR_BGR2GRAY)
        else:
            text_mask_gray = text_mask.copy()
        _, text_mask_bin = cv2.threshold(text_mask_gray, 1, 255, cv2.THRESH_BINARY)

        # æ–‡æœ¬æ©ç è†¨èƒ€
        dilate_kernel = np.ones((text_dilate_pixel * 2 + 1, text_dilate_pixel * 2 + 1), np.uint8)
        text_mask_dilated = cv2.dilate(text_mask_bin, dilate_kernel, iterations=1)

        # é‡å åŒºåŸŸæ¶‚é»‘
        text_edge_overlap = np.logical_and(edge_mask_bin == edge_white_value, text_mask_dilated == 255)
        refined_edge_mask[text_edge_overlap] = fill_black_value

    return refined_edge_mask


# -------------------------- æ¨¡å‹æ¨ç†å‡½æ•°ï¼ˆæ¯«ç§’å•ä½æ—¶é—´ç»Ÿè®¡ï¼‰ --------------------------
def run_sam_inference(img_path, sam_model, max_masks=300):
    """SAMæ¨ç†ï¼šè¿”å›è¾¹ç¼˜æ©ç æ•°ç»„ + æ¨ç†è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰"""
    try:
        # è®°å½•SAMæ¨ç†å¼€å§‹æ—¶é—´
        start_time = time.time()

        mask_generator = SamAutomaticMaskGenerator(sam_model)
        image = cv2.imread(img_path)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # ç”ŸæˆSAMæ©ç 
        masks = mask_generator.generate(image_rgb)

        # ç”Ÿæˆè¾¹ç¼˜æ©ç ï¼ˆä»…å†…å­˜æ“ä½œï¼‰
        edge_mask = np.zeros(image.shape[:2], dtype=np.uint8)
        for mask_data in masks:
            mask = mask_data["segmentation"].astype(np.uint8) * 255
            edges = cv2.Canny(mask, threshold1=50, threshold2=150)
            edge_mask = cv2.bitwise_or(edge_mask, edges)

        # è®¡ç®—SAMæ¨ç†è€—æ—¶ï¼ˆè½¬æ¢ä¸ºæ¯«ç§’ï¼Œä¿ç•™1ä½å°æ•°ï¼‰
        sam_infer_time = round((time.time() - start_time) * 1000, 1)

        # æ¸…ç†æ˜¾å­˜ç¼“å­˜ï¼ˆç¼“è§£OOMï¼‰
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return {
            "status": "success",
            "img_name": Path(img_path).stem,
            "sam_edge_mask": edge_mask,
            "sam_infer_time": sam_infer_time  # æ¯«ç§’å•ä½
        }
    except Exception as e:
        return {
            "status": "failed",
            "img_path": img_path,
            "error": str(e),
            "sam_infer_time": 0.0  # å¤±è´¥æ—¶è€—æ—¶è®°ä¸º0
        }


def run_hisam_inference(img_path, hisam_model, hier_det=False, patch_mode=False):
    """Hi-SAMæ¨ç†ï¼šè¿”å›æ–‡æœ¬æ©ç æ•°ç»„ + æ¨ç†è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰"""
    try:
        # è®°å½•Hi-SAMæ¨ç†å¼€å§‹æ—¶é—´
        start_time = time.time()

        predictor = SamPredictor(hisam_model)
        image = cv2.imread(img_path)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        img_name = Path(img_path).stem

        if patch_mode:
            # Patchæ¨¡å¼æ¨ç†
            ori_size = image.shape[:2]
            patch_list, h_slice_list, w_slice_list = patchify_sliding(image_rgb, 512, 384)
            mask_512 = []
            for patch in patch_list:
                predictor.set_image(patch)
                m, hr_m, score, hr_score = predictor.predict(multimask_output=False, return_logits=True)
                mask_512.append(hr_m[0])
            mask_512 = unpatchify_sliding(mask_512, h_slice_list, w_slice_list, ori_size)
            text_mask = (mask_512 > predictor.model.mask_threshold).astype(np.uint8) * 255
        else:
            predictor.set_image(image_rgb)
            if hier_det:
                # å±‚çº§æ£€æµ‹æ¨¡å¼
                input_point = np.array([[125, 275]])
                input_label = np.ones(input_point.shape[0])
                mask, hr_mask, score, hr_score, hi_mask, hi_iou, word_mask = predictor.predict(
                    multimask_output=False,
                    hier_det=True,
                    point_coords=input_point,
                    point_labels=input_label,
                )
                text_mask = hr_mask[0].astype(np.uint8) * 255  # è½¬ä¸º0-255çš„å•é€šé“æ©ç 
            else:
                # æ™®é€šæ–‡æœ¬åˆ†å‰²æ¨¡å¼
                mask, hr_mask, score, hr_score = predictor.predict(multimask_output=False)
                text_mask = hr_mask[0].astype(np.uint8) * 255  # è½¬ä¸º0-255çš„å•é€šé“æ©ç 

        # è®¡ç®—Hi-SAMæ¨ç†è€—æ—¶ï¼ˆè½¬æ¢ä¸ºæ¯«ç§’ï¼Œä¿ç•™1ä½å°æ•°ï¼‰
        hisam_infer_time = round((time.time() - start_time) * 1000, 1)

        # æ¸…ç†æ˜¾å­˜ç¼“å­˜ï¼ˆç¼“è§£OOMï¼‰
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return {
            "status": "success",
            "img_name": img_name,
            "hisam_text_mask": text_mask,
            "hisam_infer_time": hisam_infer_time  # æ¯«ç§’å•ä½
        }
    except Exception as e:
        return {
            "status": "failed",
            "img_path": img_path,
            "error": str(e),
            "hisam_infer_time": 0.0  # å¤±è´¥æ—¶è€—æ—¶è®°ä¸º0
        }


# -------------------------- ä¸»å‡½æ•°ï¼ˆæ¯«ç§’å•ä½æ—¶é—´ç»Ÿè®¡æ±‡æ€»ï¼‰ --------------------------
def main():
    args = get_args_parser()

    # 1. åˆ›å»ºç»“æœç›®å½•
    os.makedirs(args.output, exist_ok=True)
    print(f"ğŸ“ ç»“æœä¿å­˜ç›®å½•ï¼š{args.output}")

    # 2. åŠ è½½æ¨¡å‹
    print("\nğŸš€ åŠ è½½æ¨¡å‹...")
    sam = sam_model_registry[args.sam_model_type](checkpoint=args.sam_checkpoint)
    sam.to(device=args.device)
    sam.eval()
    hisam = model_registry[args.hisam_model_type](args)
    hisam.eval()
    hisam.to(args.device)
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    # 3. è·å–è¾“å…¥å›¾åƒåˆ—è¡¨
    input_images = []
    if os.path.isdir(args.input):
        for fname in os.listdir(args.input):
            img_path = os.path.join(args.input, fname)
            if cv2.haveImageReader(img_path):
                input_images.append(img_path)
    else:
        input_images = glob.glob(os.path.expanduser(args.input))

    assert len(input_images) > 0, "âŒ æœªæ‰¾åˆ°æœ‰æ•ˆè¾“å…¥å›¾åƒ"
    print(f"\nğŸ“¸ å¾…å¤„ç†å›¾åƒæ•°é‡ï¼š{len(input_images)}")

    # åˆå§‹åŒ–æ—¶é—´ç»Ÿè®¡å˜é‡ï¼ˆæ¯«ç§’ï¼‰
    total_sam_time = 0.0  # SAMæ€»è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
    total_hisam_time = 0.0  # Hi-SAMæ€»è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
    success_sam_count = 0  # SAMæˆåŠŸæ•°
    success_hisam_count = 0  # Hi-SAMæˆåŠŸæ•°
    time_stats = []  # å•å¼ å›¾ç‰‡è€—æ—¶æ˜ç»†

    # 4. ä¸²è¡Œè¿è¡ŒSAM + Hi-SAMæ¨ç†
    print("\nâš¡ å¼€å§‹ä¸²è¡Œæ¨ç†ï¼ˆSAM + Hi-SAMï¼‰...")
    inference_results = {}
    success_count = 0

    # é€ä¸ªå¤„ç†æ¯å¼ å›¾ç‰‡
    for img_path in tqdm(input_images, desc="æ¨ç†+ä¼˜åŒ–è¿›åº¦"):
        img_name = Path(img_path).stem
        inference_results[img_name] = {}

        # 4.1 ä¸²è¡Œæ‰§è¡ŒSAMæ¨ç†
        sam_result = run_sam_inference(
            img_path=img_path,
            sam_model=sam,
            max_masks=args.sam_max_masks
        )
        inference_results[img_name]["sam"] = sam_result

        # ç´¯åŠ SAMè€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
        if sam_result["status"] == "success":
            total_sam_time += sam_result["sam_infer_time"]
            success_sam_count += 1

        # 4.2 ä¸²è¡Œæ‰§è¡ŒHi-SAMæ¨ç†
        hisam_result = run_hisam_inference(
            img_path=img_path,
            hisam_model=hisam,
            hier_det=args.hisam_hier_det,
            patch_mode=args.hisam_patch_mode
        )
        inference_results[img_name]["hisam"] = hisam_result

        # ç´¯åŠ Hi-SAMè€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
        if hisam_result["status"] == "success":
            total_hisam_time += hisam_result["hisam_infer_time"]
            success_hisam_count += 1

        # è®°å½•å•å¼ å›¾ç‰‡è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
        time_stats.append({
            "img_name": img_name,
            "sam_time": sam_result["sam_infer_time"],
            "hisam_time": hisam_result["hisam_infer_time"],
            "sam_status": sam_result["status"],
            "hisam_status": hisam_result["status"]
        })

        # 4.3 æ©ç ä¼˜åŒ– + ä¿å­˜
        if sam_result["status"] == "success" and hisam_result["status"] == "success":
            # ç›´æ¥ä»å†…å­˜è·å–æ©ç æ•°ç»„
            sam_edge_mask = sam_result["sam_edge_mask"]
            hisam_text_mask = hisam_result["hisam_text_mask"]

            # 1. ä¿å­˜æ–‡æœ¬æ©ç 
            text_mask_path = os.path.join(args.output, f"{img_name}_hisam_text_mask.png")
            cv2.imwrite(text_mask_path, hisam_text_mask)

            # 2. å†…å­˜ä¸­ä¼˜åŒ–è¾¹ç¼˜æ©ç 
            refined_edge_mask = refine_edge_mask(
                edge_mask=sam_edge_mask,
                text_mask=hisam_text_mask,
                edge_white_value=args.edge_white_value,
                fill_black_value=args.fill_black_value,
                text_dilate_pixel=args.text_dilate_pixel
            )

            # 3. ä¿å­˜ä¼˜åŒ–åçš„è¾¹ç¼˜æ©ç 
            refined_mask_path = os.path.join(args.output, f"{img_name}_refined_edge_mask.png")
            cv2.imwrite(refined_mask_path, refined_edge_mask)

            inference_results[img_name]["refined_edge_mask_path"] = refined_mask_path
            inference_results[img_name]["hisam_text_mask_path"] = text_mask_path
            success_count += 1
        else:
            # æ‰“å°å¤±è´¥ä¿¡æ¯
            print(f"\nâš ï¸ è·³è¿‡{img_name}ï¼šSAM/Hi-SAMæ¨ç†å¤±è´¥")
            if sam_result["status"] == "failed":
                print(f"   - SAMå¤±è´¥åŸå› ï¼š{sam_result['error']}")
            if hisam_result["status"] == "failed":
                print(f"   - Hi-SAMå¤±è´¥åŸå› ï¼š{hisam_result['error']}")

        # å¼ºåˆ¶æ¸…ç†æ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()

    # 5. æ—¶é—´ç»Ÿè®¡ç»“æœè¾“å‡ºï¼ˆæ¯«ç§’å•ä½ï¼‰
    print("\n" + "-" * 60)
    print("ğŸ“Š æ¨ç†æ—¶é—´ç»Ÿè®¡ï¼ˆå•ä½ï¼šæ¯«ç§’ msï¼‰")
    print("-" * 60)
    # æ•´ä½“ç»Ÿè®¡
    print(f"æ€»å¤„ç†å›¾ç‰‡æ•°ï¼š{len(input_images)}")
    print(f"SAMæˆåŠŸæ¨ç†æ•°ï¼š{success_sam_count} | Hi-SAMæˆåŠŸæ¨ç†æ•°ï¼š{success_hisam_count}")
    print(f"SAMæ€»è€—æ—¶ï¼š{total_sam_time:.1f} ms | å¹³å‡æ¯å¼ ï¼š{total_sam_time / max(success_sam_count, 1):.1f} ms")
    print(f"Hi-SAMæ€»è€—æ—¶ï¼š{total_hisam_time:.1f} ms | å¹³å‡æ¯å¼ ï¼š{total_hisam_time / max(success_hisam_count, 1):.1f} ms")

    # å•å¼ å›¾ç‰‡æ˜ç»†ï¼ˆæ¯«ç§’å•ä½ï¼‰
    print("\nğŸ“‹ å•å¼ å›¾ç‰‡è€—æ—¶æ˜ç»†ï¼š")
    for stat in time_stats:
        status = f"SAM: {stat['sam_status']} | Hi-SAM: {stat['hisam_status']}"
        print(f"  {stat['img_name']} | SAM: {stat['sam_time']:.1f}ms | Hi-SAM: {stat['hisam_time']:.1f}ms | {status}")

    # 6. æœ€ç»ˆç»“æœè¾“å‡º
    print("\nğŸ‰ ä»»åŠ¡å®Œæˆï¼æˆåŠŸå¤„ç† {success_count}/{len(input_images)} å¼ å›¾åƒ")
    print(f"ğŸ“ ç»“æœä¿å­˜ç›®å½•ï¼š{args.output}")
    print("ğŸ“„ ä»…ä¿å­˜ä»¥ä¸‹æ–‡ä»¶ï¼š")
    print("   - {img_name}_hisam_text_mask.png: Hi-SAMæ–‡æœ¬æ©ç ")
    print("   - {img_name}_refined_edge_mask.png: ä¼˜åŒ–åçš„SAMè¾¹ç¼˜æ©ç ")


if __name__ == '__main__':
    main()