import numpy as np
import torch
import warnings
import json
from pathlib import Path
from tqdm import tqdm
from PIL import Image
import cv2
import os
import glob
import argparse
import sys
import time
from typing import Optional

# -------------------------- å¯¼å…¥æ¨¡å‹ç›¸å…³æ¨¡å— --------------------------
from FastSAM.fastsam import FastSAM, FastSAMPrompt
from Hi_SAM.hi_sam.modeling.build import model_registry
from Hi_SAM.hi_sam.modeling.predictor import SamPredictor

warnings.filterwarnings("ignore")


# -------------------------- COCOæ ¼å¼å·¥å…·å‡½æ•°ï¼ˆä¿®å¤ç‰ˆï¼‰ --------------------------
def init_coco_format():
    """åˆå§‹åŒ–COCOæ ¼å¼æ•°æ®ç»“æ„ï¼Œæ–°å¢personç±»åˆ«"""
    return {
        "info": {},
        "licenses": [],
        "categories": [
            {"id": 1, "name": "text", "supercategory": "object"},
            {"id": 2, "name": "edge", "supercategory": "object"},
            {"id": 3, "name": "object", "supercategory": "object"},
            {"id": 4, "name": "person", "supercategory": "object"}  # æ–°å¢personç±»åˆ«
        ],
        "images": [],
        "annotations": []
    }


def mask_to_coco_rle(mask):
    """å°†äºŒå€¼æ©ç è½¬æ¢ä¸ºCOCO RLEæ ¼å¼"""
    mask = mask.astype(np.uint8)
    rle = {"counts": [], "size": list(mask.shape)}
    counts = []
    prev = 0

    for pixel in mask.flatten(order='F'):
        if pixel != prev:
            counts.append(1)
            prev = pixel
        else:
            if counts:
                counts[-1] += 1
            else:
                counts.append(1)

    if not counts:
        counts = [mask.size]

    rle["counts"] = counts
    return rle


def add_coco_annotation(coco_data, img_id, mask, category_id):
    """å‘COCOæ•°æ®ä¸­æ·»åŠ æ ‡æ³¨"""
    if np.sum(mask) == 0:
        print(f"âš ï¸ è·³è¿‡ç©ºæ©ç æ ‡æ³¨ï¼ˆç±»åˆ«ID: {category_id}ï¼Œå›¾åƒID: {img_id}ï¼‰")
        return

    area = int(np.sum(mask))
    where = np.argwhere(mask)
    if len(where) == 0:
        print(f"âš ï¸ æ©ç æ— æœ‰æ•ˆåƒç´ ï¼ˆç±»åˆ«ID: {category_id}ï¼Œå›¾åƒID: {img_id}ï¼‰")
        return

    y1, x1 = where.min(axis=0)
    y2, x2 = where.max(axis=0)
    bbox = [int(x1), int(y1), int(x2 - x1 + 1), int(y2 - y1 + 1)]
    rle = mask_to_coco_rle(mask)

    annotation = {
        "id": len(coco_data["annotations"]) + 1,
        "image_id": img_id,
        "category_id": category_id,
        "segmentation": rle,
        "area": area,
        "bbox": bbox,
        "iscrowd": 0,
        "attributes": {}
    }

    coco_data["annotations"].append(annotation)


# -------------------------- å…¨å±€é…ç½®ä¸å·¥å…·å‡½æ•° --------------------------
def get_args_parser():
    parser = argparse.ArgumentParser('Fast-SAM + Hi-SAM + æ©ç ä¼˜åŒ– é«˜æ•ˆæµç¨‹', add_help=False)
    # é€šç”¨é…ç½®
    parser.add_argument("--input", type=str, required=True, help="è¾“å…¥å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„")
    parser.add_argument("--output", type=str, default='./final_results', help="ç»“æœä¿å­˜æ ¹ç›®å½•")
    parser.add_argument("--device", type=str, default="cuda:0", help="è¿è¡Œè®¾å¤‡")

    # Fast-SAMé…ç½®
    parser.add_argument("--fastsam_checkpoint", type=str, required=True, help="Fast-SAMæƒé‡è·¯å¾„")
    parser.add_argument("--fastsam_conf", type=float, default=0.4, help="Fast-SAMç½®ä¿¡åº¦é˜ˆå€¼")
    parser.add_argument("--fastsam_iou", type=float, default=0.9, help="Fast-SAM IoUé˜ˆå€¼")
    parser.add_argument("--fastsam_imgsz", type=int, default=640, help="Fast-SAMè¾“å…¥å›¾åƒå°ºå¯¸")

    # Hi-SAMé…ç½®
    parser.add_argument("--hisam_model_type", type=str, default="vit_l",
                        help="Hi-SAMæ¨¡å‹ç±»å‹ ['vit_h', 'vit_l', 'vit_b']")
    parser.add_argument("--hisam_checkpoint", type=str, required=True, help="Hi-SAMæƒé‡è·¯å¾„")
    parser.add_argument("--hisam_hier_det", action='store_true', help="Hi-SAMæ˜¯å¦å¯ç”¨å±‚çº§æ£€æµ‹")
    parser.add_argument("--hisam_patch_mode", action='store_true', help="Hi-SAMæ˜¯å¦å¯ç”¨patchæ¨¡å¼")
    parser.add_argument('--input_size', default=[1024, 1024], type=list)
    parser.add_argument('--attn_layers', default=1, type=int, help='cross attention layersæ•°')
    parser.add_argument('--prompt_len', default=12, type=int, help='prompt tokenæ•°')

    # åå¤„ç†é…ç½®
    parser.add_argument("--text_dilate_pixel", type=int, default=20, help="æ–‡æœ¬æ©ç è†¨èƒ€åƒç´ æ•°")
    parser.add_argument("--edge_white_value", type=int, default=255, help="è¾¹ç¼˜æ©ç ç™½è‰²å€¼")
    parser.add_argument("--fill_black_value", type=int, default=0, help="é‡å åŒºåŸŸå¡«å……é»‘è‰²å€¼")

    # æ–°å¢personåˆ†å‰²é…ç½®
    parser.add_argument("--person_prompt", type=str, default="person",
                        help="ç”¨äºåˆ†å‰²äººä½“çš„æ–‡æœ¬æç¤ºè¯")
    parser.add_argument("--person_conf_threshold", type=float, default=0.5,
                        help="äººä½“æ©ç ç½®ä¿¡åº¦é˜ˆå€¼")

    return parser.parse_args()


# -------------------------- Hi-SAMå·¥å…·å‡½æ•° --------------------------
def patchify_sliding(image: np.array, patch_size: int = 512, stride: int = 256):
    h, w = image.shape[:2]
    patch_list = []
    h_slice_list = []
    w_slice_list = []
    for j in range(0, h, stride):
        start_h, end_h = j, j + patch_size
        if end_h > h:
            start_h = max(h - patch_size, 0)
            end_h = h
        for i in range(0, w, stride):
            start_w, end_w = i, i + patch_size
            if end_w > w:
                start_w = max(w - patch_size, 0)
                end_w = w
            h_slice = slice(start_h, end_h)
            h_slice_list.append(h_slice)
            w_slice = slice(start_w, end_w)
            w_slice_list.append(w_slice)
            patch_list.append(image[h_slice, w_slice])
    return patch_list, h_slice_list, w_slice_list


def unpatchify_sliding(patch_list, h_slice_list, w_slice_list, ori_size):
    assert len(ori_size) == 2
    whole_logits = np.zeros(ori_size)
    assert len(patch_list) == len(h_slice_list) == len(w_slice_list)
    for idx in range(len(patch_list)):
        h_slice = h_slice_list[idx]
        w_slice = w_slice_list[idx]
        whole_logits[h_slice, w_slice] += patch_list[idx]
    return whole_logits


# -------------------------- æ©ç ä¼˜åŒ–å‡½æ•° --------------------------
def refine_edge_mask(
        edge_mask: np.ndarray,
        text_mask: Optional[np.ndarray] = None,
        edge_white_value: int = 255,
        fill_black_value: int = 0,
        text_dilate_pixel: int = 20
) -> np.ndarray:
    """ä¼˜åŒ–SAMè¾¹ç¼˜æ©ç """
    if len(edge_mask.shape) == 3:
        edge_mask_gray = cv2.cvtColor(edge_mask, cv2.COLOR_BGR2GRAY)
    else:
        edge_mask_gray = edge_mask.copy()
    _, edge_mask_bin = cv2.threshold(
        edge_mask_gray,
        edge_white_value - 1,
        edge_white_value,
        cv2.THRESH_BINARY
    )

    refined_edge_mask = edge_mask_bin.copy()

    if text_mask is not None:
        if len(text_mask.shape) == 3:
            text_mask_gray = cv2.cvtColor(text_mask, cv2.COLOR_BGR2GRAY)
        else:
            text_mask_gray = text_mask.copy()
        _, text_mask_bin = cv2.threshold(text_mask_gray, 1, 255, cv2.THRESH_BINARY)

        dilate_kernel = np.ones((text_dilate_pixel * 2 + 1, text_dilate_pixel * 2 + 1), np.uint8)
        text_mask_dilated = cv2.dilate(text_mask_bin, dilate_kernel, iterations=1)

        text_edge_overlap = np.logical_and(edge_mask_bin == edge_white_value, text_mask_dilated == 255)
        refined_edge_mask[text_edge_overlap] = fill_black_value

    return refined_edge_mask




# -------------------------- æ¨¡å‹æ¨ç†å‡½æ•° --------------------------
def run_fastsam_inference(img_path, fastsam_model, device, imgsz=1024, conf=0.4, iou=0.9):
    """Fast-SAMæ¨ç†ï¼šè¿”å›è¾¹ç¼˜æ©ç æ•°ç»„ã€åŸå§‹ç‰©ä½“æ©ç åˆ—è¡¨ + æ¨ç†è€—æ—¶"""
    try:
        start_time = time.time()
        image = cv2.imread(img_path)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        img_h, img_w = image.shape[:2]

        everything_results = fastsam_model(
            img_path,
            device=device,
            retina_masks=True,
            imgsz=imgsz,
            conf=conf,
            iou=iou
        )

        prompt_process = FastSAMPrompt(img_path, everything_results, device=device)
        ann = prompt_process.everything_prompt()

        person_masks = prompt_process.text_prompt(text="person")  # ä½¿ç”¨æ–‡æœ¬æç¤ºè¯

        # è½¬æ¢ä¸ºäºŒå€¼æ©ç å¹¶åˆå¹¶
        combined_person_mask = np.zeros((img_h, img_w), dtype=np.uint8)
        for mask in person_masks:
            mask_np = mask.astype(np.uint8) * 255
            combined_person_mask = cv2.bitwise_or(combined_person_mask, mask_np)

        edge_mask = np.zeros(image.shape[:2], dtype=np.uint8)
        object_masks = []

        for mask in ann:
            mask_np = mask.cpu().numpy().astype(np.uint8) * 255
            object_masks.append(mask_np)
            edges = cv2.Canny(mask_np, threshold1=50, threshold2=150)
            edge_mask = cv2.bitwise_or(edge_mask, edges)

        fastsam_infer_time = round((time.time() - start_time) * 1000, 1)

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return {
            "status": "success",
            "img_name": Path(img_path).stem,
            "img_size": (img_h, img_w),
            "sam_edge_mask": edge_mask,
            "object_masks": object_masks,
            "sam_infer_time": fastsam_infer_time,
            "person_mask": combined_person_mask
        }
    except Exception as e:
        return {
            "status": "failed",
            "img_path": img_path,
            "error": str(e),
            "sam_infer_time": 0.0
        }


def run_hisam_inference(img_path, hisam_model, hier_det=False, patch_mode=False):
    """Hi-SAMæ¨ç†ï¼šè¿”å›æ–‡æœ¬æ©ç æ•°ç»„ + æ¨ç†è€—æ—¶"""
    try:
        start_time = time.time()
        predictor = SamPredictor(hisam_model)
        image = cv2.imread(img_path)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        img_name = Path(img_path).stem

        if patch_mode:
            ori_size = image.shape[:2]
            patch_list, h_slice_list, w_slice_list = patchify_sliding(image_rgb, 512, 384)
            mask_512 = []
            for patch in patch_list:
                predictor.set_image(patch)
                m, hr_m, score, hr_score = predictor.predict(multimask_output=False, return_logits=True)
                mask_512.append(hr_m[0])
            mask_512 = unpatchify_sliding(mask_512, h_slice_list, w_slice_list, ori_size)
            text_mask = (mask_512 > predictor.model.mask_threshold).astype(np.uint8) * 255
        else:
            predictor.set_image(image_rgb)
            if hier_det:
                input_point = np.array([[125, 275]])
                input_label = np.ones(input_point.shape[0])
                mask, hr_mask, score, hr_score, hi_mask, hi_iou, word_mask = predictor.predict(
                    multimask_output=False,
                    hier_det=True,
                    point_coords=input_point,
                    point_labels=input_label,
                )
                text_mask = hr_mask[0].astype(np.uint8) * 255
            else:
                mask, hr_mask, score, hr_score = predictor.predict(multimask_output=False)
                text_mask = hr_mask[0].astype(np.uint8) * 255

        hisam_infer_time = round((time.time() - start_time) * 1000, 1)

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return {
            "status": "success",
            "img_name": img_name,
            "hisam_text_mask": text_mask,
            "hisam_infer_time": hisam_infer_time
        }
    except Exception as e:
        return {
            "status": "failed",
            "img_path": img_path,
            "error": str(e),
            "hisam_infer_time": 0.0
        }


# -------------------------- ä¸»å‡½æ•° --------------------------
def main():
    args = get_args_parser()

    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs(args.output, exist_ok=True)
    print(f"ğŸ“ ç»“æœä¿å­˜ç›®å½•ï¼š{args.output}")

    # åŠ è½½æ¨¡å‹
    print("\nğŸš€ åŠ è½½æ¨¡å‹...")
    fastsam = FastSAM(args.fastsam_checkpoint)
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")

    hisam = model_registry[args.hisam_model_type](args)
    hisam.eval()
    hisam.to(device)
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡ï¼š{device}")

    # è·å–è¾“å…¥å›¾åƒåˆ—è¡¨
    input_images = []
    if os.path.isdir(args.input):
        for fname in os.listdir(args.input):
            img_path = os.path.join(args.input, fname)
            if cv2.haveImageReader(img_path):
                input_images.append(img_path)
    else:
        input_images = glob.glob(os.path.expanduser(args.input))

    assert len(input_images) > 0, "âŒ æœªæ‰¾åˆ°æœ‰æ•ˆè¾“å…¥å›¾åƒ"
    print(f"\nğŸ“¸ å¾…å¤„ç†å›¾åƒæ•°é‡ï¼š{len(input_images)}")

    # åˆå§‹åŒ–æ—¶é—´ç»Ÿè®¡å˜é‡
    total_sam_time = 0.0
    total_hisam_time = 0.0
    success_sam_count = 0
    success_hisam_count = 0
    time_stats = []

    # ä¸²è¡Œè¿è¡Œæ¨ç†
    print("\nâš¡ å¼€å§‹ä¸²è¡Œæ¨ç†ï¼ˆFast-SAM + Hi-SAMåˆ†å‰²ï¼‰...")
    inference_results = {}
    success_count = 0

    for img_idx, img_path in enumerate(tqdm(input_images, desc="æ¨ç†+ä¼˜åŒ–è¿›åº¦")):
        img_name = Path(img_path).stem
        inference_results[img_name] = {}

        # åˆå§‹åŒ–COCOæ ¼å¼æ•°æ®
        coco_data = init_coco_format()

        # æ·»åŠ å›¾åƒä¿¡æ¯åˆ°COCOæ•°æ®
        img = cv2.imread(img_path)
        img_h, img_w = img.shape[:2]
        coco_data["images"].append({
            "id": img_idx + 1,
            "width": img_w,
            "height": img_h,
            "file_name": os.path.basename(img_path)
        })

        # æ‰§è¡ŒFast-SAMæ¨ç†
        sam_result = run_fastsam_inference(
            img_path=img_path,
            fastsam_model=fastsam,
            device=device,
            imgsz=args.fastsam_imgsz,
            conf=args.fastsam_conf,
            iou=args.fastsam_iou
        )
        inference_results[img_name]["sam"] = sam_result

        if sam_result["status"] == "success":
            total_sam_time += sam_result["sam_infer_time"]
            success_sam_count += 1

        # æ‰§è¡ŒHi-SAMæ¨ç†
        hisam_result = run_hisam_inference(
            img_path=img_path,
            hisam_model=hisam,
            hier_det=args.hisam_hier_det,
            patch_mode=args.hisam_patch_mode
        )
        inference_results[img_name]["hisam"] = hisam_result

        if hisam_result["status"] == "success":
            total_hisam_time += hisam_result["hisam_infer_time"]
            success_hisam_count += 1


        # è®°å½•å•å¼ å›¾ç‰‡è€—æ—¶
        time_stats.append({
            "img_name": img_name,
            "sam_time": sam_result["sam_infer_time"],
            "hisam_time": hisam_result["hisam_infer_time"],
            "sam_status": sam_result["status"],
            "hisam_status": hisam_result["status"]
        })

        # æ©ç ä¼˜åŒ– + ä¿å­˜ + ç”ŸæˆCOCOæ ‡æ³¨
        if (sam_result["status"] == "success" and
                hisam_result["status"] == "success"):
            # è·å–å„ç±»æ©ç 
            sam_edge_mask = sam_result["sam_edge_mask"]
            hisam_text_mask = hisam_result["hisam_text_mask"]
            object_masks = sam_result["object_masks"]
            person_mask = sam_result["person_mask"]  # äººä½“æ©ç 
            img_h, img_w = sam_result["img_size"]

            # 1. ä¿å­˜æ–‡æœ¬æ©ç å¹¶æ·»åŠ åˆ°COCO
            text_mask_path = os.path.join(args.output, f"{img_name}_hisam_text_mask.png")
            cv2.imwrite(text_mask_path, hisam_text_mask)
            text_mask_bin = (hisam_text_mask > 127).astype(np.uint8)
            add_coco_annotation(coco_data, img_idx + 1, text_mask_bin, 1)

            # 2. ä¼˜åŒ–è¾¹ç¼˜æ©ç å¹¶æ·»åŠ åˆ°COCO
            refined_edge_mask = refine_edge_mask(
                edge_mask=sam_edge_mask,
                text_mask=hisam_text_mask,
                edge_white_value=args.edge_white_value,
                fill_black_value=args.fill_black_value,
                text_dilate_pixel=args.text_dilate_pixel
            )
            refined_mask_path = os.path.join(args.output, f"{img_name}_refined_edge_mask.png")
            cv2.imwrite(refined_mask_path, refined_edge_mask)
            edge_mask_bin = (refined_edge_mask > 127).astype(np.uint8)
            add_coco_annotation(coco_data, img_idx + 1, edge_mask_bin, 2)

            # 3. å¤„ç†ç‰©ä½“æ©ç å¹¶æ·»åŠ åˆ°COCO
            combined_object_mask = np.zeros((img_h, img_w), dtype=np.uint8)
            for mask in object_masks:
                mask_bin = (mask > 127).astype(np.uint8)
                combined_object_mask = np.logical_or(combined_object_mask, mask_bin).astype(np.uint8)

            text_mask_dilated = cv2.dilate(text_mask_bin, np.ones((5, 5), np.uint8), iterations=1)
            exclude_mask = np.logical_or(text_mask_dilated, edge_mask_bin).astype(np.uint8)
            combined_object_mask = np.logical_and(combined_object_mask, 1 - exclude_mask).astype(np.uint8)

            object_mask_path = os.path.join(args.output, f"{img_name}_object_mask.png")
            cv2.imwrite(object_mask_path, combined_object_mask * 255)
            add_coco_annotation(coco_data, img_idx + 1, combined_object_mask, 3)

            # æ–°å¢ï¼š4. å¤„ç†äººä½“æ©ç å¹¶æ·»åŠ åˆ°COCO
            person_mask_bin = (person_mask > 127).astype(np.uint8)
            # åº”ç”¨ç½®ä¿¡åº¦é˜ˆå€¼è¿‡æ»¤
            if np.sum(person_mask_bin) > 0:
                person_mask_path = os.path.join(args.output, f"{img_name}_person_mask.png")
                cv2.imwrite(person_mask_path, person_mask)
                add_coco_annotation(coco_data, img_idx + 1, person_mask_bin, 4)  # ç±»åˆ«4: person
                inference_results[img_name]["person_mask_path"] = person_mask_path

            # ä¿å­˜COCOæ ¼å¼JSONæ–‡ä»¶
            coco_json_path = os.path.join(args.output, f"{img_name}_coco_annotations.json")
            with open(coco_json_path, 'w', encoding='utf-8') as f:
                json.dump(coco_data, f, ensure_ascii=False, indent=2)

            inference_results[img_name]["refined_edge_mask_path"] = refined_mask_path
            inference_results[img_name]["hisam_text_mask_path"] = text_mask_path
            inference_results[img_name]["object_mask_path"] = object_mask_path
            inference_results[img_name]["coco_json_path"] = coco_json_path

            success_count += 1
        else:
            print(f"\nâš ï¸ è·³è¿‡{img_name}ï¼šæ¨ç†å¤±è´¥")
            if sam_result["status"] == "failed":
                print(f"   - Fast-SAMå¤±è´¥åŸå› ï¼š{sam_result['error']}")
            if hisam_result["status"] == "failed":
                print(f"   - Hi-SAMå¤±è´¥åŸå› ï¼š{hisam_result['error']}")


        # æ¸…ç†æ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()

    # æ—¶é—´ç»Ÿè®¡ç»“æœè¾“å‡º
    print("\n" + "-" * 60)
    print("ğŸ“Š æ¨ç†æ—¶é—´ç»Ÿè®¡ï¼ˆå•ä½ï¼šæ¯«ç§’ msï¼‰")
    print("-" * 60)
    print(f"æ€»å¤„ç†å›¾ç‰‡æ•°ï¼š{len(input_images)}")
    print(
        f"Fast-SAMæˆåŠŸæ¨ç†æ•°ï¼š{success_sam_count} | Hi-SAMæˆåŠŸæ¨ç†æ•°ï¼š{success_hisam_count}")
    print(f"Fast-SAMæ€»è€—æ—¶ï¼š{total_sam_time:.1f} ms | å¹³å‡æ¯å¼ ï¼š{total_sam_time / max(success_sam_count, 1):.1f} ms")
    print(f"Hi-SAMæ€»è€—æ—¶ï¼š{total_hisam_time:.1f} ms | å¹³å‡æ¯å¼ ï¼š{total_hisam_time / max(success_hisam_count, 1):.1f} ms")

    # å•å¼ å›¾ç‰‡æ˜ç»†
    print("\nğŸ“‹ å•å¼ å›¾ç‰‡è€—æ—¶æ˜ç»†ï¼š")
    for stat in time_stats:
        status = f"Fast-SAM: {stat['sam_status']} | Hi-SAM: {stat['hisam_status']}"
        print(
            f"  {stat['img_name']} | Fast-SAM: {stat['sam_time']:.1f}ms | Hi-SAM: {stat['hisam_time']:.1f}ms")

    # æœ€ç»ˆç»“æœè¾“å‡º
    print("\nğŸ‰ ä»»åŠ¡å®Œæˆï¼æˆåŠŸå¤„ç† " + f"{success_count}/{len(input_images)} å¼ å›¾åƒ")
    print(f"ğŸ“ ç»“æœä¿å­˜ç›®å½•ï¼š{args.output}")
    print("ğŸ“„ ä¿å­˜æ–‡ä»¶åŒ…æ‹¬ï¼š")
    print("   - {img_name}_hisam_text_mask.png: Hi-SAMæ–‡æœ¬æ©ç ")
    print("   - {img_name}_refined_edge_mask.png: ä¼˜åŒ–åçš„Fast-SAMè¾¹ç¼˜æ©ç ")
    print("   - {img_name}_object_mask.png: ç‰©ä½“æ©ç ï¼ˆæ’é™¤æ–‡æœ¬å’Œè¾¹ç¼˜ï¼‰")
    print("   - {img_name}_person_mask.png: äººä½“æ©ç ï¼ˆæ–°å¢ï¼‰")  # æ–°å¢
    print("   - {img_name}_coco_annotations.json: COCOæ ¼å¼æ ‡æ³¨æ–‡ä»¶ï¼ˆåŒ…å«text/edge/object/personå››ç±»åˆ«ï¼‰")  # æ›´æ–°


if __name__ == '__main__':
    main()